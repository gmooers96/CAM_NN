{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbb906b-61d8-4eb6-ae6d-60a3250e76b8",
   "metadata": {},
   "source": [
    "# Coarse Grain the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcced93-9f68-474f-b419-52b073f5f1f7",
   "metadata": {},
   "source": [
    "The raw gSAM output data (given to us by \n",
    "Marat Khairoutdinov) can be found on Bridges2 at:\n",
    "\n",
    "/ocean/projects/ees240018p/gmooers/gsam_data/\n",
    "\n",
    "To coarse grain it go to the directory:\n",
    "\n",
    "SAM_Data_Collection/\n",
    "\n",
    "Here run the script (if not on bridges2 you will need to customize the datapaths): \n",
    "\n",
    "gsam_main_loop.py\n",
    "\n",
    "A bash script to run it is called:\n",
    "\n",
    "my_bash_script\n",
    "\n",
    "Within gsam_main_loop.py you can edit which gSAM variables to coarse-grain, and which to interpolate from z to sigma coordinates. For parallelization, you can select how many timesteps you want to be done on the single gsam_main_loop.py file vs. parallelized across multiple .py files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff84714-9798-40eb-a96b-6935f0ddfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 gsam_main_loop.py # to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffcb6f-cc82-4338-b907-376d602c9abf",
   "metadata": {},
   "source": [
    "For my purposes I set my coarse-grained (by 12x) simulation to:\n",
    "\n",
    "/ocean/projects/ees220005p/gmooers/GM_data/ (line 51 in gsam_main_loop.py)\n",
    "\n",
    "For full coarse-grained simulation production use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7934ca3-0999-4837-80e1-629abdc6d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash copy_mult_from_JY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b114f53-6723-4f98-b871-97d5ec1af9cd",
   "metadata": {},
   "source": [
    "To clone hundreds of gsam_main_loop.py files. In the bash script, set the simulation start time, the number of files to create, and the time interval between files -- see the relevant two lines from copy_mult_from_JY below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a743067-fc46-4017-add2-ac1f1e185ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ((i=2; i<874; i++)); # 873 more gsam_main_loop.py files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb08525-0d36-4d92-8565-735ff70c84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts at 137880, the interval is 3600 seconds (1 hour) -- so a separate file for each hour of raw simulation output\n",
    "sed -i \"s/start_time=137880/start_time=137880+3600*${i}/g\" ${new_mat}; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2c3ef-2fdb-4cb9-9f4b-8a91b738530f",
   "metadata": {},
   "source": [
    "# Regrid the gSAM data to CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9d900-9e04-4ad4-aed3-54df613392c1",
   "metadata": {},
   "source": [
    "Now that the relevant variables have been selected, you can regrid the information for CAM. Use regridder.py in the Regrdder/ directory. Depending on the ammount of gSAM data you preprocessed above you can either run the file directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e55ac-750d-4a9b-bc8a-af24931e70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 regridder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e198ca-4079-4931-865d-49ee2b0c3d06",
   "metadata": {},
   "source": [
    "Or submit it to the slurm queue (reccomended) via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b7d89-b2d3-4d66-b19a-cdac14b0b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sbatch bash.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185c5a5-f523-445b-9ea5-cc091fcaba8b",
   "metadata": {},
   "source": [
    "The key lines to customize are the source directory where you put your variables of interest from gSAM (line 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f179d-4685-4287-b316-d120dd8ff53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '/ocean/projects/ees240018p/gmooers/GM_Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cdff4-55d7-4d6b-841b-0f0c9e2f5b5c",
   "metadata": {},
   "source": [
    "And if you are not working on Bridges2, where you put your CAM grid information for the regridding (line 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83058a4-0ec0-4db4-b27f-d2519dac430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_file = '/ocean/projects/ees240018p/gmooers/CAM/aqua_sst_YOG_f09.cam.h0.0001-04-01-00000.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078b762-2a5d-4432-b247-2dd66d20aa3c",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed68043-c7e4-4bac-a69a-bd91a6a28221",
   "metadata": {},
   "source": [
    "In this stage, our specific inputs and outputs for the neural network are derived from the coarse-grained simulation data created in the two stages above. The output will be a series of .nc files with these input/output variables. Note -- this is just the derivation of the variables -- scaling occurs later.\n",
    "\n",
    "The essential script can be found in the directory:\n",
    "\n",
    "NN_Input_Output_Data/input_output_data_preparation.py\n",
    "\n",
    "The python script will be populated by yaml files. These are found at:\n",
    "\n",
    "NN_Input_Output_Data/configs/\n",
    "\n",
    "You can see two different configs:\n",
    "\n",
    "config_1_CAM.yaml \\\n",
    "config_2_CAM.yaml \n",
    "\n",
    "Where I use config file 1 for training and config file 2 for testing.\n",
    "\n",
    "When populated by one of the config files above, the python script goes through the coarse-grained simulation create the input/output variables. In more detail, here are the flags in a given .yaml file:\n",
    "\n",
    "An example of how to generate the training and test data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512738f-8cbf-4b3f-8ef8-5565991fb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 input_output_data_preparation.py configs/config_1_CAM.yaml\n",
    "! python3 input_output_data_preparation.py configs/config_2_CAM.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9743b1-0d33-444e-9151-6d790d5897c5",
   "metadata": {},
   "source": [
    "However, for any significant data volume, I recommend submitting the job to the slurm que via the bashscripts for training and testing data found in the Bash_Scripts/ directory within NN_Input_Output_Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b86af-d250-466d-8b1b-d05da454f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The key parts of the config to update are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17123417-370f-4e07-87cc-9a535cc8622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the coarse-grained data is coming from (this will select all 877 .nc files)\n",
    "filepath: \"/ocean/projects/ees240018p/gmooers/Regridding/Regridded_Data/*0000[012]**.nc4\"\n",
    "# where the inputs/outputs will be saved to\n",
    "savepath: \"/ocean/projects/ees240018p/gmooers/Regridding/Training_Data/\"\n",
    "# how many vertical levels in the column to use in the inputs/outputs\n",
    "levels: 49\n",
    "# if getting terrain information, what level to take  -- should this be 0?\n",
    "ground_levels: 1\n",
    "# identifying name\n",
    "my_name: \"Input_Output_Data_Training_Part_1_\"\n",
    "# if True, get additional information for outputs (std / std_min) for scaling for the NN\n",
    "rewight_outputs: False\n",
    "\n",
    "#below are what variables to put in the input and output vector\n",
    "# Tin = Absolute Temperature of the column at each level in (K)\n",
    "# Qin = QC (Cloud Water) + QV (Water Vapor) + QI (Cloud Ice) in (g/kg)\n",
    "# skt = skin temperature at the given atmospheric temperature\n",
    "# cos_lat = cosine of the latitude at the given atmospheric column\n",
    "# sfc_pres = surface reference pressure -- proxy for terrain at a given atmospheric column\n",
    "# land_frac = land or ocean (1 or 0) at a given atmospheric column\n",
    "# predict tendencies -- set to True to make script create your output vector of fluxes/tends\n",
    "# grid cell area -- meters squared area of cell on the cam grid\n",
    "\n",
    "flag_dict:\n",
    "  Tin_feature: True\n",
    "  qin_feature: True\n",
    "  predict_tendencies: True\n",
    "  skt: False\n",
    "  land_frac: True\n",
    "  sfc_pres: True\n",
    "  cos_lat: False\n",
    "  grid_cell_area: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ba6f9-b2bd-486a-8aa4-c50286500ff2",
   "metadata": {},
   "source": [
    "# Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea886a87-96cc-4e05-bb80-eed96e113848",
   "metadata": {},
   "source": [
    "When an experiment is launched, three things happen:\n",
    "\n",
    "1). Final preparations for the data are made \\\n",
    "2). The final form of the training/test data is passed to the neural network and it is trained \\\n",
    "3). Post-processing analysis is conducted to examine the network performance\n",
    "\n",
    "You can launch an experiment with two things:\n",
    "\n",
    "1). The python script ml_train_script_netcdf.py (named because it works with prepared data in the form of .nc files) \\\n",
    "2). a .yaml config file specifying what final preparations will be to the data and hyperparameter choices for the neural network\n",
    "\n",
    "This portion of the workflow occurs in the directory Training/.\n",
    "\n",
    "An example of an experiment launch from the command line looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5ba45-8622-4be4-b57f-17a353451ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 neural_network_training_Original_JN.py New_Configs/config_10_original_CAM.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8b39b-ed27-450c-8635-27e70f58a3ce",
   "metadata": {},
   "source": [
    "With any significant amount of data, it should never be run without a cpu or gpu resource and it is more strategic to submit in a bashscript. You can find an example at\n",
    "\n",
    "Bash_Scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc686c-15b3-4d8d-834d-d0a6cf2a4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sbatch bashcpu_cam_original_10.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba78062-961a-4a4a-a69c-931411933f92",
   "metadata": {},
   "source": [
    "However, within the bash script make sure your paths are correct for your:\n",
    "\n",
    "1). Python environment\n",
    "2). Github repository\n",
    "\n",
    "Full example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42d7f8-825c-489a-8a6a-013a917d8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -A ees220005p\n",
    "#SBATCH --job-name=\"original_CAM_trial_10\"\n",
    "#SBATCH -o \"outputs/original_CAM_trial_10.%j.%N.out\"\n",
    "#SBATCH -p RM-512 #could do RM-512, RM-shared, RM\n",
    "#SBATCH -N 1\n",
    "#SBATCH --ntasks-per-node=128\n",
    "#SBATCH --export=ALL\n",
    "#SBATCH -t 72:00:00 # max of 48 hours for GPU\n",
    "#SBATCH --mem=492G\n",
    "#SBATCH --no-requeue\n",
    "\n",
    "module purge\n",
    "\n",
    "source /jet/home/gmooers/miniconda3/bin/activate torchenv\n",
    "\n",
    "cd /ocean/projects/ees240018p/gmooers/Githubs/Neural_nework_parameterization/NN_training/src/\n",
    "\n",
    "python3 neural_network_training_Original_JN.py /ocean/projects/ees240018p/gmooers/Githubs/Neural_nework_parameterization/NN_training/run_training/Improved_run_Experiments/Config_Files/New_Configs/config_10_original_CAM.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f9275-bc3d-43b8-960a-1ee6d300d073",
   "metadata": {},
   "source": [
    "This will succsessfully launch an experiment. However, for more detail on what is going on during the run, see below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd952bf-5244-4508-b147-fb765f9690ef",
   "metadata": {},
   "source": [
    "## Part 1: Final Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea1940-1a6e-42a6-b534-626295bdaa54",
   "metadata": {},
   "source": [
    "When an experiment is launched, the first thing that happens is the data is transformed once more. The specifics of this will be dictated by some of the flags in the .yaml file (New_Configs/).\n",
    "\n",
    "These include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec89a7-e5cc-43fc-833d-713e721f66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the path to the training data\n",
    "training_expt : \"/ocean/projects/ees240018p/gmooers/Regridding/Training_Data/CAM_Trial_Data_TTTTFTF_Train.nc\"\n",
    "# sets the path to the test data\n",
    "test_expt : '/ocean/projects/ees240018p/gmooers/Regridding/Training_Data/CAM_Trial_Data_TTTTFTF_Test.nc'\n",
    "# sets the path for extra weighting in the scaling of the variables\n",
    "weights_path : null # or path\n",
    "# sets what variables will be in the neural network input vector\n",
    "input_vert_vars : ['Tin','qin','terra','sfc_pres'] \n",
    "# sets what variables will be in the neural network output vector\n",
    "output_vert_vars : ['Tout', 'T_adv_out','q_adv_out','q_auto_out','q_sed_flux_tot']\n",
    "# If True, nothing. If False, removes the poles from the training data (70N-90N; 70S-90S)\n",
    "poles: True\n",
    "# what percentage of the trainng data to use\n",
    "training_data_volume: 25.0\n",
    "# what percentage of the test data to use\n",
    "test_data_volume: 50.0\n",
    "# if True, use the weights in the scaling of the data; If False, nothing\n",
    "rewight_outputs: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1ae74-bb4b-4c3d-bf0d-7417d5a884e3",
   "metadata": {},
   "source": [
    "All the above information will be passed into the training script neural_network_training_Original_JN.py, however, the details of the work happen in the LoadDataStandardScaleData_v4() function of data_scalar_and_reshaper_original.py which is called into neural_network_training_Original_JN.py in the beginning.\n",
    "\n",
    "A simplified version of the logic and steps of LoadDataStandardScaleData_v4() is depicted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a7de6-d8da-4d07-8f14-bb160e9e6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xarray opens the data -- this approach avoids loading into memory (both train + test)\n",
    "train_store = xr.open_mfdataset(traindata)\n",
    "\n",
    "# inputs (and afterwards using the same logic outputs) are extracted from the .nc files -- below showing inputs\n",
    "# training and test following the same logic as well -- but will show training below\n",
    "for inputs in input_vert_vars:\n",
    "    # array is of shape (z dimension, sample) a.k.a. (z, lat*time*lon)\n",
    "    train_dask_array = train_store[inputs]\n",
    "    # inputs are scaled (x - mean / std) by vertical level except for scalars; outputs are scaled neglecting vertical level\n",
    "    # see normalize_by_level(); standardize_outputs() helper function for details on implementation\n",
    "    scaled_train_dask_array, mean, std = normalize_by_level(train_dask_array)\n",
    "    # data is spliced based on given training/test data percentage you wish to use\n",
    "    scaled_train_dask_array = scaled_train_dask_array[:, :training_data_percentage]\n",
    "    # if the input is a scalar (land frac, sfc pres), it is reshaped from (sample) to (1,sample) -- not relevant for outputs\n",
    "    scaled_train_dask_array = scaled_train_dask_array.expand_dims(dim='new_dim', axis=0)\n",
    "    # saved to a dictionary\n",
    "    scaled_inputs['train'][inputs] = scaled_train_dask_array\n",
    "\n",
    "# after the dictionary of inputs (or outputs) of training (or test) data has been built, change it to a single DataArray Structure\n",
    "# This DataArray structure is concatenated along the Z axis (axis 0)\n",
    "# Example dictionary of Tin (49, 1000000), Qin (49,1000000), land_frac (1, 1000000), sfc_pres (1, 1000000) ==> DataArray (100, 1000000)\n",
    "# see convert_dict_to_array() helper function for implementation details\n",
    "final_train_inputs = convert_dict_to_array(scaled_inputs['train'])\n",
    "\n",
    "# swap the axis so the final array is (sample, input number) to work with the pytorch NN\n",
    "final_train_inputs = final_train_inputs.swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d16ad-6a27-47b0-a099-cadc8a58bf7f",
   "metadata": {},
   "source": [
    "The above does not cover optional flags including special weighting and removal of the poles which are also part of the full LoadDataStandardScaleData_v4() function if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268c1ad-e0e7-443a-8957-edd49ca56388",
   "metadata": {},
   "source": [
    "## Part 2: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e63f1-203c-47c4-ab54-e3408381d9f6",
   "metadata": {},
   "source": [
    "Training details are also primarily set in the .yaml config. Relevant flags are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031ff5e-bf46-445d-adcd-997190055e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the number of hidden layers in the NN and the number of neurons (size) of each layer\n",
    "layer_sizes: [1024, 512, 256, 128, 64]\n",
    "# ensures unbiased result by selecting a random seed\n",
    "random_seed: 42\n",
    "# number of epochs to train NN\n",
    "epochs: 7\n",
    "# the learning rate for the NN\n",
    "lr: 0.0000001\n",
    "# NN batch size\n",
    "batch_size: 1024 \n",
    "# set True to train a NN; False assumes NN was trained previously\n",
    "train_new_model: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1deb71-5cd6-4613-bbfc-bc6d70a90de1",
   "metadata": {},
   "source": [
    "Below are some of the key steps in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ac6fb-e98e-4138-956a-f26a42b4a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the script itself the model information is instantiated by the CustomNN() class\n",
    "# input/output size determined by the outputs of LoadDataStandardScaleData_v4() higher up in code\n",
    "model = CustomNN(input_size, layer_sizes, output_size)\n",
    "\n",
    "# machine identifies if CPU or GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "# Use DataParallel for multiple GPUs -- this allows for the option of future paralleization -- not currently leveraged\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Creates a diagram of the model structure for visualization\n",
    "model_graph = draw_graph(model, \n",
    "                                input_size=(batch_size, input_size),\n",
    "                                graph_name=nametag,\n",
    "                                save_graph=True,\n",
    "                                directory=path_for_design,\n",
    "                                    )\n",
    "\n",
    "# starting point standard loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model training is handled in function new_train_model()\n",
    "if train_new_model is True:\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies, avg_epoch_time = new_train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=epochs, device=device)\n",
    "\n",
    "    # Plot the losses and accuracies for each epoch\n",
    "    plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies, avg_epoch_time, name, save_path, nametag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadba814-0e25-4bbb-8429-7332696ee8d3",
   "metadata": {},
   "source": [
    "After successfully passing the training code the model now enters post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d28593-936b-4212-bfcb-c1711fdb1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the test data from the trained model\n",
    "scaled_predictions = get_model_predictions_from_numpy(model=model, #trained model\n",
    "                                     test_data=test_inputs_scaled, #scaled test data\n",
    "                                     batch_size=batch_size,\n",
    "                                                         )\n",
    "# unscale the predictions to compare to the outputs\n",
    "unscaled_predictions = undo_scaling(\n",
    "                                scaled_array=scaled_predictions, \n",
    "                                scaler_dict=train_outputs_pp_dict, \n",
    "                                vertical_dimension=levels, \n",
    "                                output_variables=output_vert_vars,\n",
    "                                )\n",
    "\n",
    "# plotting and analysis   \n",
    "post_processing_figures.main_plotting(truth=np.transpose(test_outputs_original), # put z dimension first for plotting code\n",
    "                                              pred=np.transpose(unscaled_predictions), #put z dimension first for plotting code \n",
    "                                              raw_data=single_file,\n",
    "                                              z_dim=levels,\n",
    "                                             var_names=output_vert_vars,\n",
    "                                              save_path=save_path,\n",
    "                                              nametag=nametag,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f964d-ef51-439f-b1d6-391d948bb534",
   "metadata": {},
   "source": [
    "Go to the savepath from the config file (example below) andf see how your neural network performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ee682-8f1e-4813-a8b7-a9349d77bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path: '/ocean/projects/ees240018p/gmooers/Investigations/Model_Performance/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
